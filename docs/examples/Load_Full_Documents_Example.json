{
  "format": "pedantic-workflow-v1",
  "metadata": {
    "name": "Load Full Documents Example",
    "description": "Example workflow showing how to load full documents (from files, URLs, or Python) into the vector database. Supports large documents and optional chunking.",
    "tags": ["RAG", "full-documents", "file-loading", "chunking", "example"],
    "exportedAt": "2025-01-29T00:00:00.000Z",
    "version": "1.0.0"
  },
  "workflow": {
    "nodes": {
      "start": {
        "type": "start",
        "title": "Start",
        "position": { "x": 100, "y": 200 }
      },
      "python_load_docs": {
        "type": "python",
        "title": "Load Documents from Files",
        "code": "def run(input):\n    import os\n    from pathlib import Path\n    \n    # Option 1: Load from files in /tmp/workflow_files/\n    docs_dir = Path('/tmp/workflow_files/documents')\n    documents = []\n    \n    if docs_dir.exists():\n        for file_path in docs_dir.glob('*.txt'):  # or .md, .json, etc.\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n                documents.append({\n                    'id': len(documents) + 1,\n                    'filename': file_path.name,\n                    'content': content,\n                    'size': len(content)\n                })\n    \n    # Option 2: Load from a single large file\n    # large_file = Path('/tmp/workflow_files/document.txt')\n    # if large_file.exists():\n    #     with open(large_file, 'r', encoding='utf-8') as f:\n    #         content = f.read()\n    #         documents.append({\n    #             'id': 1,\n    #             'filename': large_file.name,\n    #             'content': content,\n    #             'size': len(content)\n    #         })\n    \n    # Option 3: Load from URLs (using HTTP node first, then process)\n    # This would require an HTTP node before this Python node\n    \n    return {\n        'documents': documents,\n        'count': len(documents),\n        'total_size': sum(d.get('size', 0) for d in documents)\n    }",
        "position": { "x": 300, "y": 200 }
      },
      "python_chunk_docs": {
        "type": "python",
        "title": "Chunk Large Documents (Optional)",
        "code": "def run(input):\n    documents = input.get('documents', [])\n    chunk_size = 1000  # Characters per chunk\n    chunk_overlap = 200  # Overlap between chunks\n    \n    chunked_docs = []\n    doc_id = 1\n    \n    for doc in documents:\n        content = doc.get('content', '')\n        filename = doc.get('filename', 'unknown')\n        \n        # For very large documents, split into chunks\n        if len(content) > chunk_size:\n            # Split into overlapping chunks\n            start = 0\n            while start < len(content):\n                end = start + chunk_size\n                chunk = content[start:end]\n                \n                chunked_docs.append({\n                    'id': doc_id,\n                    'filename': filename,\n                    'chunk_index': len([c for c in chunked_docs if c.get('filename') == filename]),\n                    'content': chunk,\n                    'size': len(chunk)\n                })\n                doc_id += 1\n                \n                # Move start position with overlap\n                start = end - chunk_overlap\n                if start >= len(content):\n                    break\n        else:\n            # Small document, keep as-is\n            chunked_docs.append({\n                'id': doc_id,\n                'filename': filename,\n                'chunk_index': 0,\n                'content': content,\n                'size': len(content)\n            })\n            doc_id += 1\n    \n    return {\n        'documents': chunked_docs,\n        'count': len(chunked_docs),\n        'original_count': len(documents)\n    }",
        "position": { "x": 500, "y": 200 }
      },
      "database_setup": {
        "type": "database",
        "title": "Setup Database",
        "config": {
          "operation": "create",
          "database": "full_docs.db",
          "query": "CREATE TABLE IF NOT EXISTS docs (id INTEGER PRIMARY KEY, filename TEXT, content TEXT, size INTEGER, chunk_index INTEGER, embedding BLOB);"
        },
        "position": { "x": 700, "y": 200 }
      },
      "database_load_ext": {
        "type": "database",
        "title": "Load Extension",
        "config": {
          "operation": "select",
          "database": "full_docs.db",
          "query": "SELECT load_extension('/tmp/workflow_files/vec0.dylib');"
        },
        "position": { "x": 900, "y": 200 }
      },
      "database_vec_table": {
        "type": "database",
        "title": "Create Vector Table",
        "config": {
          "operation": "select",
          "database": "full_docs.db",
          "query": "CREATE VIRTUAL TABLE IF NOT EXISTS vec_docs USING vec0(embedding float[384] distance_metric=cosine);"
        },
        "position": { "x": 1100, "y": 200 }
      },
      "foreach_docs": {
        "type": "foreach",
        "title": "Process Documents",
        "config": {
          "items_key": "documents",
          "execution_mode": "serial",
          "max_concurrency": 1
        },
        "position": { "x": 1300, "y": 200 }
      },
      "embedding_doc": {
        "type": "embedding",
        "title": "Generate Embedding",
        "config": {
          "model": "all-MiniLM-L6-v2",
          "input_field": "content",
          "output_field": "embedding",
          "format": "blob"
        },
        "position": { "x": 1500, "y": 300 }
      },
      "database_insert": {
        "type": "database",
        "title": "Store in DB",
        "config": {
          "operation": "insert",
          "database": "full_docs.db",
          "query": "INSERT INTO docs (filename, content, size, chunk_index, embedding) VALUES (?, ?, ?, ?, ?); INSERT INTO vec_docs(rowid, embedding) VALUES (last_insert_rowid(), ?);",
          "params": ["{filename}", "{content}", "{size}", "{chunk_index}", "{embedding}", "{embedding}"]
        },
        "position": { "x": 1700, "y": 300 }
      },
      "endloop": {
        "type": "endloop",
        "title": "End Loop",
        "position": { "x": 1900, "y": 300 }
      },
      "python_query": {
        "type": "python",
        "title": "User Query",
        "code": "def run(input):\n    query = 'What information is in the documents?'\n    return {'query': query, 'user_query': query, 'content': query}",
        "position": { "x": 2100, "y": 200 }
      },
      "embedding_query": {
        "type": "embedding",
        "title": "Embed Query",
        "config": {
          "model": "all-MiniLM-L6-v2",
          "input_field": "content",
          "output_field": "query_embedding",
          "format": "blob"
        },
        "position": { "x": 2300, "y": 200 }
      },
      "database_search": {
        "type": "database",
        "title": "Vector Search",
        "config": {
          "operation": "select",
          "database": "full_docs.db",
          "query": "SELECT v.rowid, v.distance, d.filename, d.content, d.chunk_index FROM vec_docs v JOIN docs d ON v.rowid = d.id WHERE v.embedding MATCH ? AND k = 5;",
          "params": ["{query_embedding}"]
        },
        "position": { "x": 2500, "y": 200 }
      },
      "python_format": {
        "type": "python",
        "title": "Format Context",
        "code": "def run(input):\n    results = input.get('data', [])\n    query = input.get('user_query', input.get('query', 'Unknown query'))\n    if 'SELECT' in str(query) or 'FROM' in str(query):\n        query = 'Unknown query'\n    \n    # Format with filename and chunk info\n    context_parts = []\n    for i, row in enumerate(results):\n        if isinstance(row, dict):\n            filename = row.get('filename', 'unknown')\n            content = row.get('content', '')\n            distance = row.get('distance', 0)\n            chunk_index = row.get('chunk_index', 0)\n            \n            source_info = f\"{filename}\"\n            if chunk_index > 0:\n                source_info += f\" (chunk {chunk_index})\"\n            \n            context_parts.append(f\"[Source {i+1}: {source_info}] (similarity: {1-distance:.3f})\\n{content}\")\n    \n    context = '\\n\\n'.join(context_parts)\n    \n    return {\n        'query': query,\n        'user_query': query,\n        'context': context,\n        'retrieved_count': len(results)\n    }",
        "position": { "x": 2700, "y": 200 }
      },
      "llm_answer": {
        "type": "llm",
        "title": "Generate Answer",
        "config": {
          "provider": "openrouter",
          "model": "openai/gpt-4o-mini",
          "temperature": 0.3,
          "max_tokens": 500,
          "system": "You are a helpful assistant. Answer the user's question using ONLY the provided context. Cite sources with [Source X: filename].",
          "user": "Question: {query}\\n\\nContext:\\n{context}\\n\\nAnswer:",
          "api_key_name": "OPENROUTER_API_KEY"
        },
        "position": { "x": 2900, "y": 200 }
      },
      "markdown_output": {
        "type": "markdown",
        "title": "View Answer",
        "config": {
          "content_key": "content"
        },
        "position": { "x": 3100, "y": 200 }
      },
      "end": {
        "type": "end",
        "title": "End",
        "position": { "x": 3300, "y": 200 }
      }
    },
    "connections": {
      "start_to_load": {
        "source": "start",
        "target": "python_load_docs",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "load_to_chunk": {
        "source": "python_load_docs",
        "target": "python_chunk_docs",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "chunk_to_setup": {
        "source": "python_chunk_docs",
        "target": "database_setup",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "setup_to_load_ext": {
        "source": "database_setup",
        "target": "database_load_ext",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "load_ext_to_vec_table": {
        "source": "database_load_ext",
        "target": "database_vec_table",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "vec_table_to_foreach": {
        "source": "database_vec_table",
        "target": "foreach_docs",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "foreach_to_embed": {
        "source": "foreach_docs",
        "target": "embedding_doc",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "embed_to_insert": {
        "source": "embedding_doc",
        "target": "database_insert",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "insert_to_endloop": {
        "source": "database_insert",
        "target": "endloop",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "endloop_to_query": {
        "source": "endloop",
        "target": "python_query",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "query_to_embed_query": {
        "source": "python_query",
        "target": "embedding_query",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "embed_query_to_search": {
        "source": "embedding_query",
        "target": "database_search",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "search_to_format": {
        "source": "database_search",
        "target": "python_format",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "format_to_llm": {
        "source": "python_format",
        "target": "llm_answer",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "llm_to_markdown": {
        "source": "llm_answer",
        "target": "markdown_output",
        "sourceOutput": "output",
        "targetInput": "input"
      },
      "markdown_to_end": {
        "source": "markdown_output",
        "target": "end",
        "sourceOutput": "output",
        "targetInput": "input"
      }
    }
  }
}

